"""
streamlit_app.py

Streamlit app that consumes artifacts generated by model_pipeline.py and provides:
- upload dataset (CSV/XLSX) and run predictions, or
- provide a single-row CSV/text input for ad-hoc prediction
- shows predicted class + probability (from saved best_model)
- displays SHAP images and embedded LIME explanation (single instance)
- allows download of predictions CSV

Run:
    streamlit run streamlit_app.py
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import json
import os
import base64

# artifact paths (must match train_pipeline output)
ARTIFACT_DIR = "artifacts"
BEST_MODEL_PATH = os.path.join(ARTIFACT_DIR, "best_model.joblib")
SCALER_PATH = os.path.join(ARTIFACT_DIR, "scaler.joblib")
ENCODERS_PATH = os.path.join(ARTIFACT_DIR, "label_encoders.joblib")
FEATURE_LIST_PATH = os.path.join(ARTIFACT_DIR, "feature_list.json")
MODEL_PERF_PATH = os.path.join(ARTIFACT_DIR, "model_performance.json")
SHAP_SUMMARY_PNG = os.path.join(ARTIFACT_DIR, "shap_summary.png")
LIME_HTML = os.path.join(ARTIFACT_DIR, "lime_explanation.html")
FI_PNG = os.path.join(ARTIFACT_DIR, "feature_importance.png")

st.set_page_config(page_title="E-Learning Adoption Predictor", layout="wide")

st.title("E-Learning Adoption Predictor (Full Interpretability Mode)")
st.markdown("Upload dataset or provide a single-row input. Predictions use the saved pipeline artifacts.")

# Check artifacts presence
missing = []
for p in [BEST_MODEL_PATH, SCALER_PATH, ENCODERS_PATH, FEATURE_LIST_PATH]:
    if not os.path.exists(p):
        missing.append(p)
if missing:
    st.error(f"Missing artifacts: {missing}. Run `model_pipeline.py` first to generate them.")
    st.stop()

# Load artifacts
best_model = joblib.load(BEST_MODEL_PATH)
scaler = joblib.load(SCALER_PATH)
label_encoders = joblib.load(ENCODERS_PATH)
with open(FEATURE_LIST_PATH, "r") as f:
    feature_list = json.load(f)

st.sidebar.header("Prediction Mode")
mode = st.sidebar.selectbox("Mode", ["Upload dataset (batch predict)", "Single-row input (CSV line)"])

def preprocess_apply_streamlit(df_in):
    # same logic as training's apply_preprocessing but simplified for Streamlit
    df_local = df_in.copy()
    # drop timestamp/time columns
    df_local = df_local.drop(columns=[c for c in df_local.columns if 'timestamp' in c.lower() or 'time' in c.lower()], errors='ignore')
    # fill missing
    for col in df_local.columns:
        if df_local[col].isnull().sum() > 0:
            if df_local[col].dtype == object:
                df_local[col].fillna(df_local[col].mode()[0] if len(df_local[col].mode())>0 else "Unknown", inplace=True)
            else:
                df_local[col].fillna(df_local[col].median(), inplace=True)
    # apply label encoders where applicable
    for col, le in label_encoders.items():
        if col in df_local.columns:
            df_local[col] = df_local[col].astype(str).map(lambda x: x if x in le.classes_ else le.classes_[0])
            df_local[col] = le.transform(df_local[col].astype(str))
    # encode remaining object columns on the fly
    remaining_obj = df_local.select_dtypes(include=['object']).columns.tolist()
    for col in remaining_obj:
        df_local[col] = df_local[col].astype(str).factorize()[0]
    # feature engineering (PU, PEOU, BI) - keyword based
    cols = df_local.columns.tolist()
    pu_keywords = ['useful', 'facilitate', 'improve', 'enhance', 'benefit']
    pu_cols = [c for c in cols if any(k in c.lower() for k in pu_keywords)]
    if pu_cols and 'perceived_usefulness_score' not in df_local.columns:
        df_local['perceived_usefulness_score'] = df_local[pu_cols].mean(axis=1)
    peou_keywords = ['easy', 'simple', 'convenient', 'effort']
    peou_cols = [c for c in cols if any(k in c.lower() for k in peou_keywords)]
    if peou_cols and 'perceived_ease_score' not in df_local.columns:
        df_local['perceived_ease_score'] = df_local[peou_cols].mean(axis=1)
    bi_keywords = ['willing', 'intend', 'want', 'plan']
    bi_cols = [c for c in cols if any(k in c.lower() for k in bi_keywords)]
    if bi_cols and 'willingness_score' not in df_local.columns:
        df_local['willingness_score'] = df_local[bi_cols].mean(axis=1)
    # ensure numeric and fillna
    df_local = df_local.apply(pd.to_numeric, errors='coerce')
    df_local = df_local.fillna(df_local.median())
    # align to feature_list
    for feat in feature_list:
        if feat not in df_local.columns:
            df_local[feat] = 0.0
    df_local = df_local[feature_list]
    # scale
    X_scaled = scaler.transform(df_local)
    return df_local, X_scaled

if mode == "Upload dataset (batch predict)":
    uploaded = st.file_uploader("Upload CSV or Excel with the same columns used in training", type=['csv','xlsx','xls'])
    if uploaded is not None:
        try:
            if uploaded.name.endswith(".csv"):
                df_in = pd.read_csv(uploaded)
            else:
                df_in = pd.read_excel(uploaded)
            st.write("Preview of uploaded data (first 5 rows):")
            st.dataframe(df_in.head())
            if st.button("Run predictions on uploaded dataset"):
                with st.spinner("Preprocessing and predicting..."):
                    df_local, X_scaled = preprocess_apply_streamlit(df_in)
                    preds = best_model.predict(X_scaled)
                    probs = best_model.predict_proba(X_scaled)[:,1] if hasattr(best_model, "predict_proba") else np.zeros(len(preds))
                    result_df = df_local.copy()
                    result_df["predicted_adopter"] = preds
                    result_df["probability_adopter"] = probs
                    st.success("Predictions complete.")
                    st.dataframe(result_df.head())
                    # download
                    csv = result_df.to_csv(index=False)
                    b64 = base64.b64encode(csv.encode()).decode()
                    href = f'<a href="data:file/csv;base64,{b64}" download="predictions.csv">Download predictions.csv</a>'
                    st.markdown(href, unsafe_allow_html=True)
                    # show interpretability artifacts (SHAP & LIME)
                    st.markdown("---")
                    st.subheader("Global Interpretability (SHAP Summary)")
                    if os.path.exists(SHAP_SUMMARY_PNG):
                        st.image(SHAP_SUMMARY_PNG, use_container_width=True)
                    if os.path.exists(FI_PNG):
                        st.markdown("Feature importance (from training):")
                        st.image(FI_PNG, use_container_width=True)
                    st.subheader("Local Explainability (LIME)")
                    if os.path.exists(LIME_HTML):
                        with open(LIME_HTML, "r", encoding="utf-8") as f:
                            html = f.read()
                        st.components.v1.html(html, height=480, scrolling=True)
        except Exception as e:
            st.error(f"Failed to process uploaded file: {e}")

else:
    st.write("Provide a single-row CSV line (header + values) or paste CSV content for one sample. The columns should match training instrument.")
    sample_text = st.text_area("Paste single-row CSV (header + one row). Example:\nSelect your Gender,Select your age range, ...\nMale,18-24,...", height=180)
    if st.button("Predict single instance") and sample_text.strip():
        try:
            # read the CSV content
            from io import StringIO
            df_in = pd.read_csv(StringIO(sample_text.strip()))
            if df_in.shape[0] != 1:
                st.warning("Please provide exactly one data row.")
            else:
                df_local, X_scaled = preprocess_apply_streamlit(df_in)
                pred = best_model.predict(X_scaled)[0]
                prob = best_model.predict_proba(X_scaled)[0,1] if hasattr(best_model, "predict_proba") else None
                st.markdown("### Prediction")
                st.write("Predicted adopter (1=Yes,0=No):", int(pred))
                st.write("Predicted probability of adoption:", float(prob) if prob is not None else "N/A")
                st.markdown("---")
                st.subheader("Interpretability artifacts (from training)")
                if os.path.exists(SHAP_SUMMARY_PNG):
                    st.image(SHAP_SUMMARY_PNG, use_container_width=True)
                if os.path.exists(FI_PNG):
                    st.image(FI_PNG, use_container_width=True)
                st.subheader("LIME explanation (representative instance)")
                if os.path.exists(LIME_HTML):
                    with open(LIME_HTML, "r", encoding="utf-8") as f:
                        html = f.read()
                    st.components.v1.html(html, height=480, scrolling=True)
        except Exception as e:
            st.error(f"Error predicting single instance: {e}")

st.markdown("---")
st.caption("Streamlit app expects artifacts produced by model_pipeline.py. If you modify training code, re-run model_pipeline.py to regenerate artifacts.")
